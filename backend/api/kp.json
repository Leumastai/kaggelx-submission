{"qeury": "Why should they be a Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks?", "answer": " According to the text, Batch Normalization and Dropout are commonly used techniques to increase the training efficiency of deep neural networks. However, using them together may not be possible with either technique alone. Therefore, rethinking their usage can lead to new training techniques that can improve the training efficiency of DNNs.", "response": {"question": "Why should they be a Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks?", "chat_history": [["Why should they be a Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks?", " According to the text, Batch Normalization and Dropout are commonly used techniques to increase the training efficiency of deep neural networks. However, using them together may not be possible with either technique alone. Therefore, rethinking their usage can lead to new training techniques that can improve the training efficiency of DNNs."]], "answer": " According to the text, Batch Normalization and Dropout are commonly used techniques to increase the training efficiency of deep neural networks. However, using them together may not be possible with either technique alone. Therefore, rethinking their usage can lead to new training techniques that can improve the training efficiency of DNNs.", "source_documents": [{"qeury": "Why should they be a Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks?", "answer": " According to the text, Batch Normalization and Dropout are commonly used techniques to increase the training efficiency of deep neural networks. However, using them together may not be possible with either technique alone. Therefore, rethinking their usage can lead to new training techniques that can improve the training efficiency of DNNs.", "response": {"question": "Why should they be a Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks?", "chat_history": [["Why should they be a Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks?", " According to the text, Batch Normalization and Dropout are commonly used techniques to increase the training efficiency of deep neural networks. However, using them together may not be possible with either technique alone. Therefore, rethinking their usage can lead to new training techniques that can improve the training efficiency of DNNs."]], "answer": " According to the text, Batch Normalization and Dropout are commonly used techniques to increase the training efficiency of deep neural networks. However, using them together may not be possible with either technique alone. Therefore, rethinking their usage can lead to new training techniques that can improve the training efficiency of DNNs.", "source_documents": [{"page_content": "Rethinking the Usage of Batch Normalization and Dropout\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich,\nA. Going deeper with convolutions. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition , pp. 1\u20139, 2015.\nUlyanov, D., Vedaldi, A., and Lempitsky, V . S. Instance\nnormalization: The missing ingredient for fast stylization.\nCoRR , abs/1607.08022, 2016. URL http://arxiv.\norg/abs/1607.08022 .\nWu, Y . and He, K. Group normalization. arXiv preprint\narXiv:1803.08494 , 2018.", "metadata": {"doc_id": "bc399b4b-7d58-484e-9a52-4874eaa6bda0", "page": 9.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/bc399b4b-7d58-484e-9a52-4874eaa6bda0.pdf"}}, {"page_content": "neurons and millions of parameters. Ef\ufb01cient training of a\nmodern DNN is often complicated by the need of feeding\nsuch a behemoth with millions of data entries. Developing\nnovel techniques to increase training ef\ufb01ciency of DNNs is\na highly active research topics. In this work, we propose a\nnovel training technique by combining two commonly used\nones, Batch Normalization (BatchNorm) (Ioffe & Szegedy,\n2015) and Dropout (Srivastava et al., 2014), for a purpose\n(making independent inputs to neural networks) that is not\npossibly achieved by either technique alone. This marriage", "metadata": {"doc_id": "bc399b4b-7d58-484e-9a52-4874eaa6bda0", "page": 0.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/bc399b4b-7d58-484e-9a52-4874eaa6bda0.pdf"}}, {"page_content": "Rethinking the Usage of Batch Normalization and Dropout\nTable 1. The testing results of implementing ResNet and ResNet-B with the IC layer on the CIFAR10/100 datasets at the end of training.\nModel Depth Layers in Residual Unit CIFAR10 CIFAR100\nResNet1101\u20dd.2\u00d7{ReLU-IC-Conv2D} 0.9361 0.725\n2\u20dd.2\u00d7{IC-Conv2D-ReLU} 0.9352 0.7165\n3\u20dd.2\u00d7{Conv2D-ReLU-IC} 0.9408 0.7174\nBaseline 0.9292 0.6893\n1641\u20dd.2\u00d7{ReLU-IC-Conv2D} 0.9395 0.7224\n2\u20dd.2\u00d7{IC-Conv2D-ReLU} 0.9366 0.7273\n3\u20dd.2\u00d7{Conv2D-ReLU-IC} 0.9411 0.7237\nBaseline 0.9339 0.6981\nResNet-B1101\u20dd.3\u00d7{ReLU-IC-Conv2D} 0.9448 0.7563\n2\u20dd.3\u00d7{IC-Conv2D-ReLU} 0.9425 0.7532", "metadata": {"doc_id": "bc399b4b-7d58-484e-9a52-4874eaa6bda0", "page": 5.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/bc399b4b-7d58-484e-9a52-4874eaa6bda0.pdf"}}, {"page_content": "Rethinking the Usage of Batch Normalization and Dropout\nby its symmetric counterpart, namely hyperbolic tangent.\nWhile in modern applications where ReLU is more powerful\nand common, the issue still remains as explained above and\ncan still be found in our experiments. The similar results\ncan be also observed in Fig. 5, where the IC layer stabi-\nlizes the training process and achieves better generalization\nperformance especially in the \ufb01rst 30epochs.\n6. Conclusions and Future Works\nIn this paper, we rethink the usage of BatchNorm and\nDropout in the training of DNNs, and \ufb01nd that they should", "metadata": {"doc_id": "bc399b4b-7d58-484e-9a52-4874eaa6bda0", "page": 8.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/bc399b4b-7d58-484e-9a52-4874eaa6bda0.pdf"}}, {"page_content": "Rethinking the Usage of Batch Normalization and Dropout\n\u2022To corroborate our theoretical analysis, we con-\nduct extensive experiments on CIFAR10/100 and\nILSVRC2012 datasets. The results convincingly ver-\nify that our implementation improves the classi\ufb01cation\nperformance of modern networks in three ways: i)\nmore stable training process , ii)faster convergence\nspeed , and iii) better convergence limit .\n2. Preliminary: the In\ufb02uence of Uncorrelated\nComponents\nIn this section, we discuss the in\ufb02uence of uncorrelated\ninputs on the training of DNNs. Following (Le Cun et al.,", "metadata": {"doc_id": "bc399b4b-7d58-484e-9a52-4874eaa6bda0", "page": 2.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/bc399b4b-7d58-484e-9a52-4874eaa6bda0.pdf"}}], "generated_question": "Why should they be a Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks?"}}