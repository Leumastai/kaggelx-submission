{"data": {"qeury": "What are the top tips for training transformers?", "answer": " Based on the provided text, here are some top tips for training transformers:\n\n1. Make sure that the subword vocabulary is trained on a sufficiently large sample of the training data.\n2. Exclude too long sentences during preprocessing to avoid divergence in the early training phase.\n3. Increase the batch size for better training and use a higher learning rate.\n4. Decrease the warmup steps if the model is difficult to initialize or if the warmup steps are too high relative to the expected total training steps.\n5. Use multiple GPUs on the same machine to speed up training.", "system_response": {"question": "What are the top tips for training transformers?", "chat_history": [["What are the top tips for training transformers?", " Based on the provided text, here are some top tips for training transformers:\n\n1. Make sure that the subword vocabulary is trained on a sufficiently large sample of the training data.\n2. Exclude too long sentences during preprocessing to avoid divergence in the early training phase.\n3. Increase the batch size for better training and use a higher learning rate.\n4. Decrease the warmup steps if the model is difficult to initialize or if the warmup steps are too high relative to the expected total training steps.\n5. Use multiple GPUs on the same machine to speed up training."]], "answer": " Based on the provided text, here are some top tips for training transformers:\n\n1. Make sure that the subword vocabulary is trained on a sufficiently large sample of the training data.\n2. Exclude too long sentences during preprocessing to avoid divergence in the early training phase.\n3. Increase the batch size for better training and use a higher learning rate.\n4. Decrease the warmup steps if the model is difficult to initialize or if the warmup steps are too high relative to the expected total training steps.\n5. Use multiple GPUs on the same machine to speed up training.", "source_documents": [[{"page_content": "Martin Popel, Ond\u0159ej Bojar Training Tips for the Transformer Model (43\u201370)\n..0\n.5\n.10\n.15\n.20\n.0\n.5\n.10\n.15\n.20\n.25\n.30\n.35\n.40\n.45\n.\nBLEU\n.Training time (hours).learning rate 0.25\n.learning rate 0.20\n.learning rate 0.10\n.learning rate 0.05\n.learning rate 0.01\nFigure 7: Effect of the learning rate on a single GPU. All trained on CzEng 1.0 with\nthe default batch size (1500) and warmup steps (16k).\ndifficult to initialize and thus more sensitive to divergence in the early training phase.\nAlso while for BASE, increasing the batch size was highly helpful until 4500, for BIG", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 16.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "Cite as: Martin Popel, Ond\u0159ej Bojar. Training Tips for the Transformer Model. The Prague Bulletin of Mathematical\nLinguistics No. 110, 2018, pp. 43\u201370. doi: 10.2478/pralin-2018-0002.", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 0.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "Tips on Training Data Preprocessing\n\u2022 Make sure that the subword vocabulary is trained on a sufficiently large sample\nof the training data.12\n\u2022 As discussed in Section 4.5 , a higher batch size may be beneficial for the train-\ning and the batch size can be higher when excluding training sentences longer\nthan a given threshold. This can be controlled with parameter max_length (see\nSection 4.4 ), but it may be a good idea to exclude too long sentences even before\npreparing the training data using t2t-datagen . This way the TFRecords training", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 6.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "Martin Popel, Ond\u0159ej Bojar Training Tips for the Transformer Model (43\u201370)\ntional-tokenization ) and automatically downloads the reference translation for a\ngiven WMT testset.\n2.1. Considerations on Stopping Criterion\nThe situation in NMT is further complicated by the fact that the training of NMT\nsystems is usually non-deterministic,4and (esp. with the most recent models) hardly\never converges or starts overfitting5on reasonably big datasets. This leads to learning\ncurves that never fully flatten let alone start decreasing (see Section 4.2 ). The common", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 2.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "Martin Popel, Ond\u0159ej Bojar Training Tips for the Transformer Model (43\u201370)\n\u2022 If that does not help (or if the warmup steps are too high relative to the expected\ntotal training steps), try decreasing the learning rate.\n\u2022 Note that when you decrease warmup steps (and keep learning rate), you also\nincrease the maximum actual learning rate because of the way how the lin-\near_warmup_rsqrt_decay (aka noam ) schedule is implemented.26\n4.7. Number of GPUs\nT2T allows to train with multiple GPUs on the same machine simply using the", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 18.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}]], "generated_question": "What are the top tips for training transformers?"}}, "timestamp": "2023-10-28T10:23:40.491930"}
{"data": {"qeury": "who are the authors of the paper?", "answer": " The authors of the paper are Martin Popel and Ondrej Bojar.", "system_response": {"question": "who are the authors of the paper?", "chat_history": [["What are the top tips for training transformers?", " Based on the provided text, here are some top tips for training transformers:\n\n1. Make sure that the subword vocabulary is trained on a sufficiently large sample of the training data.\n2. Exclude too long sentences during preprocessing to avoid divergence in the early training phase.\n3. Increase the batch size for better training and use a higher learning rate.\n4. Decrease the warmup steps if the model is difficult to initialize or if the warmup steps are too high relative to the expected total training steps.\n5. Use multiple GPUs on the same machine to speed up training."], ["who are the authors of the paper?", " The authors of the paper are Martin Popel and Ondrej Bojar."]], "answer": " The authors of the paper are Martin Popel and Ondrej Bojar.", "source_documents": [[{"page_content": "Martin Popel, Ond\u0159ej Bojar Training Tips for the Transformer Model (43\u201370)\neditors, T ext, Speech, and Dialogue: 19th International Conference, TSD 2016 , number 9924 in\nLecture Notes in Artificial Intelligence, pages 231\u2013238. Masaryk University , Springer Inter-\nnational Publishing, 2016. ISBN 978-3-319-45509-9.\nBojar , Ond\u0159ej, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow , Matthias\nHuck, Philipp Koehn, V arvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael\nRubino, Lucia Specia, and Marco T urchi. Findings of the 2017 Conference on Machine", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 26.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "2015. ACL. URL http://aclweb.org/anthology/W15-3049 .\nSennrich, Rico, Barry Haddow , and Alexandra Birch. Neural Machine Translation of Rare\nW ords with Subword Units. In Proceedings of ACL 2016 , pages 1715\u20131725, Berlin, Germany ,\nAugust 2016. ACL. URL http://www.aclweb.org/anthology/P16-1162 .\nShazeer, N. and M. Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost.\nArXiv e-prints , Apr. 2018. URL https://arxiv.org/abs/1804.04235 .\nSmith, Samuel L. and Quoc V . Le. A Bayesian Perspective on Generalization and Stochastic", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 27.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "PBML 110 APRIL 2018\n2.2. Our Final Choice: Full Learning Curves\nBased on the discussion above, we decided to report always the full learning curves\nand not just single scores. This solution does not fully prevent the risk of premature\njudgments, but the readers can at least judge for themselves if they would expect any\nsudden twist in the results or not.\nIn all cases, we plot the case-insensitive BLEU score against the wall-clock time in\nhours. This solution obviously depends on the hardware chosen, so we always used", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 3.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "ral Information Processing Systems 30 , pages 6000\u20136010. Curran Associates, Inc., 2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\nWu, Y onghui, Mike Schuster , Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang\nMacherey , Maxim Krikun, Y uan Cao, Qin Gao, Klaus Macherey , Jeff Klingner, Apurva\nShah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Y oshikiyo Kato,\nT aku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei W ang,\nCliff Y oung, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Mac-", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 27.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "PBML 110 APRIL 2018\nLei Ba, J., J. R. Kiros, and G. E. Hinton. Layer Normalization. ArXiv e-prints , July 2016.\nPapineni, Kishore, Salim Roukos, T odd W ard, and Wei-Jing Zhu. BLEU: a Method for Au-\ntomatic Evaluation of Machine Translation. In Proceedings of ACL 2002 , pages 311\u2013318,\nPhiladelphia, Pennsylvania, 2002.\nPopovi\u0107, Maja. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the\nT enth Workshop on Statistical Machine T ranslation , pages 392\u2013395, Lisbon, Portugal, September\n2015. ACL. URL http://aclweb.org/anthology/W15-3049 .", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 27.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}]], "generated_question": " Who are the authors of the paper?"}}, "timestamp": "2023-10-28T10:24:14.879756"}
{"data": {"qeury": "Who are the authors of the paper?", "answer": " The authors of the paper \"Training Transformers\" are Martin Popel and Ondrej Bojar.", "system_response": {"question": "Who are the authors of the paper?", "chat_history": [["What are the top tips for training transformers?", " Based on the provided text, here are some top tips for training transformers:\n\n1. Make sure that the subword vocabulary is trained on a sufficiently large sample of the training data.\n2. Exclude too long sentences during preprocessing to avoid divergence in the early training phase.\n3. Increase the batch size for better training and use a higher learning rate.\n4. Decrease the warmup steps if the model is difficult to initialize or if the warmup steps are too high relative to the expected total training steps.\n5. Use multiple GPUs on the same machine to speed up training."], ["who are the authors of the paper?", " The authors of the paper are Martin Popel and Ondrej Bojar."], ["Who are the authors of the paper?", " The authors of the paper \"Training Transformers\" are Martin Popel and Ondrej Bojar."]], "answer": " The authors of the paper \"Training Transformers\" are Martin Popel and Ondrej Bojar.", "source_documents": [[{"page_content": "Martin Popel, Ond\u0159ej Bojar Training Tips for the Transformer Model (43\u201370)\neditors, T ext, Speech, and Dialogue: 19th International Conference, TSD 2016 , number 9924 in\nLecture Notes in Artificial Intelligence, pages 231\u2013238. Masaryk University , Springer Inter-\nnational Publishing, 2016. ISBN 978-3-319-45509-9.\nBojar , Ond\u0159ej, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow , Matthias\nHuck, Philipp Koehn, V arvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael\nRubino, Lucia Specia, and Marco T urchi. Findings of the 2017 Conference on Machine", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 26.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "with self-attentional networks (Transformer). There are several other differences as\nwell: Modern convolutional networks are usually trained with batch normalization\n29T o close the gap between small-batch training and large-batch training, Hoffer et al. (2017 ) introduce (in\naddition top\nkscaling) so-called ghost batch normalization and adapted training regime , which means decaying\nthe learning rate after a given number of steps instead of epochs.\n63", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 20.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "with our best Transformer model (BIG trained on 8 GPUs for 8 days, averaging 8\ncheckpoints) evaluated using the exact same implementation of automatic metrics.\nWhile the automatic evaluation is not fully reliable (see e.g. the high BLEU score for\nCU-Chimera despite its lower manual rank), we see that the Transformer model out-\n67", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 24.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "duff Hughes, and Jeffrey Dean. Google\u2019s Neural Machine Translation System: Bridging\nthe Gap between Human and Machine Translation. CoRR , abs/1609.08144, 2016. URL\nhttp://arxiv.org/abs/1609.08144 .\nY ou, Y ang, Igor Gitman, and Boris Ginsburg. Scaling SGD Batch Size to 32K for ImageNet\nTraining. CoRR , abs/1708.03888, 2017. URL http://arxiv.org/abs/1708.03888 .\nAddress for correspondence:\nMartin Popel\npopel@ufal.mff.cuni.cz\nInstitute of Formal and Applied Linguistics\nFaculty of Mathematics and Physics, Charles University\nMalostransk\u00e9 n\u00e1m\u011bst\u00ed 25, 118 00 Praha 1\nCzech Republic\n70", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 27.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}, {"page_content": "Martin Popel, Ond\u0159ej Bojar Training Tips for the Transformer Model (43\u201370)\nsteps, but this means after 8 times less training examples. For the inverse-square-root\ndecay , we have actual_lr (steps) =c\u0001steps-0:5=1p\n8\u0001actual_lr (steps\u00018), where cis a\nconstant containing also the learning_rate parameter. So with 8 GPUs, if we divide\nthelearning_rate parameter byp\n8, we achieve the same actual learning rate after a\ngiven number of training examples as in the original single-GPU setting.\nThis explains the riddle from the previous section. By keeping the learning_rate", "metadata": {"doc_id": "1a4f98c4-fe9c-419a-b9e1-818c07d44bf5", "page": 22.0, "source": "/home/unicconai/Documents/sam's_lab/archive/delete/kagglex/backend/db/input/1a4f98c4-fe9c-419a-b9e1-818c07d44bf5.pdf"}}]], "generated_question": " Who are the authors of the paper \"Training Transformers\"?"}}, "timestamp": "2023-10-28T10:25:17.394916"}
